{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"segementation images Unet Pytorch.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Odfy-M8bTIjy"},"source":["#Medical image segmentation\n","Medical image segmentation, identifying the pixels of organs or lesions from background medical images such as CT or MRI images, is one of the most challenging tasks in medical image analysis that is to deliver critical information about the shapes and volumes of these organs."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxYNqot3vGCW","executionInfo":{"elapsed":24216,"status":"ok","timestamp":1638529487324,"user":{"displayName":"Youness EL BRAG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtRxzu5DdV8ntFlalQyGLGS9UseGRNTl5JYMSPJg=s64","userId":"18435412878697928901"},"user_tz":0},"outputId":"ccd49c73-7851-479e-aa74-b16d6763402e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHHDyIm0vGlp","executionInfo":{"elapsed":310,"status":"ok","timestamp":1638529489234,"user":{"displayName":"Youness EL BRAG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtRxzu5DdV8ntFlalQyGLGS9UseGRNTl5JYMSPJg=s64","userId":"18435412878697928901"},"user_tz":0},"outputId":"7d41a586-3c87-4e32-af88-10f8baa30c9e"},"source":["import os\n","os.listdir('/content/drive/My Drive/new_data/')"],"execution_count":null,"outputs":[{"data":{"text/plain":["['train', 'test']"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"vB1YvFseUn2E"},"source":["# Study Case :\n","our last model for multi-labels learning we got a good\n","performance from the model CNN , to gives us the probabilistic of each label\n","in singal image . but here was an issue with classfier has not labeling when it\n","comes with tow classes ( Normal , Gloumia ) because the features with these\n","classes are similary to each other , so here we purpose a solution for improve\n","the mechanism of extracting features by using segmentation images technics and the goal beind Unet is creating a new database segemented Classes to classify , and comparing the results"]},{"cell_type":"markdown","metadata":{"id":"4gaQiy5nVvcv"},"source":["Setup all functionalities being used for Data Preparation\n","\n","Images in the training dataset had differing sizes, therefore images had to be resized before being used as input to the model.\n","\n","Square images were resized to the shape 256×256 pixels. Rectangular images were resized to 256 pixels on their shortest side, then the middle 256×256 square was cropped from the image. Note: the network expects input images to have the shape 512x512, achieved via training augmentation"]},{"cell_type":"code","metadata":{"id":"EMY0D1BavGoX"},"source":["import os\n","import time\n","import random\n","import numpy as np\n","import cv2\n","import torch\n","\n","\"\"\" Seeding the randomness. \"\"\"\n","def seeding(seed):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","\"\"\" Create a directory. \"\"\"\n","def create_dir(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","\"\"\" Calculate the time taken \"\"\"\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cse6dRlEuzNM"},"source":["import os\n","import numpy as np\n","import cv2\n","import torch\n","from torch.utils.data import Dataset\n","\n","class DriveDataset(Dataset):\n","    def __init__(self, images_path, masks_path):\n","\n","        self.images_path = images_path\n","        self.masks_path = masks_path\n","        self.n_samples = len(images_path)\n","\n","    def __getitem__(self, index):\n","        \"\"\" Reading image \"\"\"\n","        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n","        image = image/255.0 ## (512, 512, 3)\n","        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)\n","        image = image.astype(np.float32)\n","        image = torch.from_numpy(image)\n","\n","        \"\"\" Reading mask \"\"\"\n","        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n","        mask = mask/255.0   ## (512, 512)\n","        mask = np.expand_dims(mask, axis=0) ## (1, 512, 512)\n","        mask = mask.astype(np.float32)\n","        mask = torch.from_numpy(mask)\n","\n","        return image, mask\n","\n","    def __len__(self):\n","        return self.n_samples"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SrUvGhTnXmv4"},"source":["# Loss Function DiceLoss\n","\n","\n","The Dice coefficient tells you how well your model is performing when it comes to detecting boundaries with regards to your ground truth data. The loss is computed with 1 - Dice coefficient where the the dice coefficient is between 0-1.\n","\n","Over every epoch the loss will determine the acceleration of learning and the updates of weights to reduce the loss as much as possible. The dice coefficient also takes into account global and local composition of pixels, thereby providing better boundary detection than a weighted cross entropy. link https://arxiv.org/pdf/1608.04117.pdf\n"]},{"cell_type":"code","metadata":{"id":"5kESNPs8veo6"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","\n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = torch.sigmoid(inputs)\n","\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","\n","        return 1 - dice\n","\n","class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceBCELoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","\n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = torch.sigmoid(inputs)\n","\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        Dice_BCE = BCE + dice_loss\n","\n","        return Dice_BCE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OqxwBaTET2yz"},"source":["#U-Net 2D U-Net\n","One of the most well-known structures for medical image segmentation is U-Net, initially proposed by Ronneberger et al. using the concept of deconvolution introduced by. This model is built upon the elegant architecture of FCN. Besides the increased depth of network to 19 layers, U-Net benefits from a superior design of skip connections between different stages of the network. It employs some modifications to overcome the trade-off between localization and the use of context. This trade-off rises since the large-sized patches require more pooling layers and consequently will reduce the localization accuracy. On the other hand, small-sized patches can only observe small context of input. The proposed structure consists of two paths of analysis and synthesis. The analysis path follows the structure of CNN (see Fig. 4). The synthesis path, commonly known as expansion phase, consists of an upsampling layer followed by a deconvolution layer. The most important property of U-Net is the shortcut connections between the layers of equal resolution in analysis path to expansion path. These connections provides essential high-resolution features to the deconvolution layers. link https://towardsdatascience.com/u-net-b229b32b4a71"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Huz3uOlves5","executionInfo":{"elapsed":15991,"status":"ok","timestamp":1638529542941,"user":{"displayName":"Youness EL BRAG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtRxzu5DdV8ntFlalQyGLGS9UseGRNTl5JYMSPJg=s64","userId":"18435412878697928901"},"user_tz":0},"outputId":"593d3c0c-5674-4903-91ed-0621ed463455"},"source":["import torch\n","import torch.nn as nn\n","\n","class conv_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_c)\n","\n","        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_c)\n","\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, inputs):\n","        x = self.conv1(inputs)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        return x\n","\n","class encoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.conv = conv_block(in_c, out_c)\n","        self.pool = nn.MaxPool2d((2, 2))\n","\n","    def forward(self, inputs):\n","        x = self.conv(inputs)\n","        p = self.pool(x)\n","\n","        return x, p\n","\n","class decoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n","        self.conv = conv_block(out_c+out_c, out_c)\n","\n","    def forward(self, inputs, skip):\n","        x = self.up(inputs)\n","        x = torch.cat([x, skip], axis=1)\n","        x = self.conv(x)\n","        return x\n","\n","class build_unet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        \"\"\" Encoder \"\"\"\n","        self.e1 = encoder_block(3, 64)\n","        self.e2 = encoder_block(64, 128)\n","        self.e3 = encoder_block(128, 256)\n","        self.e4 = encoder_block(256, 512)\n","\n","        \"\"\" Bottleneck \"\"\"\n","        self.b = conv_block(512, 1024)\n","\n","        \"\"\" Decoder \"\"\"\n","        self.d1 = decoder_block(1024, 512)\n","        self.d2 = decoder_block(512, 256)\n","        self.d3 = decoder_block(256, 128)\n","        self.d4 = decoder_block(128, 64)\n","\n","        \"\"\" Classifier \"\"\"\n","        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n","\n","    def forward(self, inputs):\n","        \"\"\" Encoder \"\"\"\n","        s1, p1 = self.e1(inputs)\n","        s2, p2 = self.e2(p1)\n","        s3, p3 = self.e3(p2)\n","        s4, p4 = self.e4(p3)\n","\n","        \"\"\" Bottleneck \"\"\"\n","        b = self.b(p4)\n","\n","        \"\"\" Decoder \"\"\"\n","        d1 = self.d1(b, s4)\n","        d2 = self.d2(d1, s3)\n","        d3 = self.d3(d2, s2)\n","        d4 = self.d4(d3, s1)\n","\n","        outputs = self.outputs(d4)\n","\n","        return outputs\n","\n","if __name__ == \"__main__\":\n","    x = torch.randn((2, 3, 512, 512))\n","    f = build_unet()\n","    y = f(x)\n","    print(y.shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 1, 512, 512])\n"]}]},{"cell_type":"markdown","metadata":{"id":"-tCrgdP0aG73"},"source":["# trainig Model\n","Loss calculation in UNet\n","\n","What kind of loss one would use in such an intrinsic image segmentation? Well, it is defined simply in the paper itself.\n","\n","    The energy function is computed by a pixel-wise soft-max over the final feature map combined with the Dice-Loss loss function\n","\n","UNet uses a rather novel loss weighting scheme for each pixel such that there is a higher weight at the border of segmented objects. This loss weighting scheme helped the U-Net model segment eyes diseases in medical images in a discontinuous fashion such that individual cells may be easily identified within the binary segmentation map.\n","\n","First of all pixel-wise softmax applied on the resultant image which is followed by  Dice-Loss loss function. So we are classifying each pixel into one of the classes. The idea is that even in segmentation every pixel have to lie in some category and we just need to make sure that they do. So we just converted a segmentation problem into a binary-class classification one and it performed very well as compared to the traditional loss functions."]},{"cell_type":"code","metadata":{"id":"zPauMnuD0rpH"},"source":["import torch.backends.cudnn as cudnn\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GE9uJCj8dLnG"},"source":["**Save the model**\n","\n","Now, lets save the model, so later we can reload and make predicions without the need to retrain. The model is then converted to JSON format and written to model.json in the local directory. The network weights are written to model.pth in the local directory."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2kuUHCcvev9","executionInfo":{"elapsed":127053,"status":"ok","timestamp":1638531452381,"user":{"displayName":"Youness EL BRAG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtRxzu5DdV8ntFlalQyGLGS9UseGRNTl5JYMSPJg=s64","userId":"18435412878697928901"},"user_tz":0},"outputId":"b871e256-5a6f-4606-e7b8-4adf82f92a5a"},"source":["import os\n","import time\n","from glob import glob\n","\n","import torch\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","\n","#from data import DriveDataset\n","#from model import build_unet\n","#from loss import DiceLoss, DiceBCELoss\n","#from utils import seeding, create_dir, epoch_time\n","\n","def train(model, loader, optimizer, loss_fn, device):\n","    epoch_loss = 0.0\n","\n","    model.train()\n","    for x, y in loader:\n","        x = x.to(device, dtype=torch.float32)\n","        y = y.to(device, dtype=torch.float32)\n","\n","        optimizer.zero_grad()\n","        y_pred = model(x)\n","        loss = loss_fn(y_pred, y)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    epoch_loss = epoch_loss/len(loader)\n","    return epoch_loss\n","\n","def evaluate(model, loader, loss_fn, device):\n","    epoch_loss = 0.0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device, dtype=torch.float32)\n","            y = y.to(device, dtype=torch.float32)\n","\n","            y_pred = model(x)\n","            loss = loss_fn(y_pred, y)\n","            epoch_loss += loss.item()\n","\n","        epoch_loss = epoch_loss/len(loader)\n","    return epoch_loss\n","\n","if __name__ == \"__main__\":\n","    \"\"\" Seeding \"\"\"\n","    seeding(42)\n","\n","    \"\"\" Directories \"\"\"\n","    create_dir(\"files\")\n","\n","    \"\"\" Load dataset \"\"\"\n","    train_x = sorted(glob(\"/content/drive/My Drive/new_data/train/image/*\"))\n","    train_y = sorted(glob(\"/content/drive/My Drive/new_data/train/mask/*\"))\n","\n","    valid_x = sorted(glob(\"/content/drive/My Drive/new_data/test/image/*\"))\n","    valid_y = sorted(glob(\"/content/drive/My Drive/new_data/test/mask/*\"))\n","\n","    data_str = f\"Dataset Size:\\nTrain: {len(train_x)} - Valid: {len(valid_x)}\\n\"\n","    print(data_str)\n","\n","    \"\"\" Hyperparameters \"\"\"\n","    H = 512\n","    W = 512\n","    size = (H, W)\n","    batch_size = 2\n","    num_epochs = 50\n","    lr = 1e-4\n","    checkpoint_path = \"/content/drive/My Drive/files/checkpoint.pth\"\n","\n","    \"\"\" Dataset and loader \"\"\"\n","    train_dataset = DriveDataset(train_x, train_y)\n","    valid_dataset = DriveDataset(valid_x, valid_y)\n","\n","    train_loader = DataLoader(\n","        dataset=train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=2\n","    )\n","\n","    valid_loader = DataLoader(\n","        dataset=valid_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2\n","    )\n","   \n","\n","    device = torch.device('cuda')   ## GTX 1060 6GB\n","    model = build_unet()\n","    model = model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n","    loss_fn = DiceBCELoss()\n","\n","    \"\"\" Training the model \"\"\"\n","    best_valid_loss = float(\"inf\")\n","\n","    for epoch in range(num_epochs):\n","        start_time = time.time()\n","\n","        train_loss = train(model, train_loader, optimizer, loss_fn, device)\n","        valid_loss = evaluate(model, valid_loader, loss_fn, device)\n","\n","        \"\"\" Saving the model \"\"\"\n","        if valid_loss < best_valid_loss:\n","            data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n","            print(data_str)\n","\n","            best_valid_loss = valid_loss\n","            torch.save(model.state_dict(), checkpoint_path)\n","\n","        end_time = time.time()\n","        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","        data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n","        data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n","        data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n","        print(data_str)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset Size:\n","Train: 20 - Valid: 20\n","\n","Valid loss improved from inf to 1.4844. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 01 | Epoch Time: 0m 31s\n","\tTrain Loss: 1.328\n","\t Val. Loss: 1.484\n","\n","Valid loss improved from 1.4844 to 1.4416. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 02 | Epoch Time: 0m 27s\n","\tTrain Loss: 1.174\n","\t Val. Loss: 1.442\n","\n","Valid loss improved from 1.4416 to 1.4034. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 03 | Epoch Time: 0m 27s\n","\tTrain Loss: 1.078\n","\t Val. Loss: 1.403\n","\n","Valid loss improved from 1.4034 to 1.3186. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 04 | Epoch Time: 0m 28s\n","\tTrain Loss: 1.043\n","\t Val. Loss: 1.319\n","\n","Valid loss improved from 1.3186 to 1.1644. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 05 | Epoch Time: 0m 28s\n","\tTrain Loss: 1.011\n","\t Val. Loss: 1.164\n","\n","Valid loss improved from 1.1644 to 1.0527. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 06 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.986\n","\t Val. Loss: 1.053\n","\n","Valid loss improved from 1.0527 to 0.9879. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 07 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.965\n","\t Val. Loss: 0.988\n","\n","Valid loss improved from 0.9879 to 0.9466. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 08 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.955\n","\t Val. Loss: 0.947\n","\n","Valid loss improved from 0.9466 to 0.9338. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 09 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.937\n","\t Val. Loss: 0.934\n","\n","Valid loss improved from 0.9338 to 0.9215. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 10 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.925\n","\t Val. Loss: 0.921\n","\n","Valid loss improved from 0.9215 to 0.9169. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 11 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.914\n","\t Val. Loss: 0.917\n","\n","Valid loss improved from 0.9169 to 0.9022. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 12 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.902\n","\t Val. Loss: 0.902\n","\n","Epoch: 13 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.888\n","\t Val. Loss: 0.903\n","\n","Valid loss improved from 0.9022 to 0.8794. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 14 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.874\n","\t Val. Loss: 0.879\n","\n","Valid loss improved from 0.8794 to 0.8764. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 15 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.863\n","\t Val. Loss: 0.876\n","\n","Valid loss improved from 0.8764 to 0.8617. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 16 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.854\n","\t Val. Loss: 0.862\n","\n","Valid loss improved from 0.8617 to 0.8399. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 17 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.839\n","\t Val. Loss: 0.840\n","\n","Valid loss improved from 0.8399 to 0.8383. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 18 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.831\n","\t Val. Loss: 0.838\n","\n","Valid loss improved from 0.8383 to 0.8254. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 19 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.816\n","\t Val. Loss: 0.825\n","\n","Valid loss improved from 0.8254 to 0.8210. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 20 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.808\n","\t Val. Loss: 0.821\n","\n","Valid loss improved from 0.8210 to 0.8032. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 21 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.795\n","\t Val. Loss: 0.803\n","\n","Valid loss improved from 0.8032 to 0.7986. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 22 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.785\n","\t Val. Loss: 0.799\n","\n","Valid loss improved from 0.7986 to 0.7963. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 23 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.775\n","\t Val. Loss: 0.796\n","\n","Valid loss improved from 0.7963 to 0.7626. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 24 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.763\n","\t Val. Loss: 0.763\n","\n","Epoch: 25 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.752\n","\t Val. Loss: 0.765\n","\n","Epoch: 26 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.740\n","\t Val. Loss: 0.766\n","\n","Valid loss improved from 0.7626 to 0.7595. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 27 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.731\n","\t Val. Loss: 0.760\n","\n","Valid loss improved from 0.7595 to 0.7538. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 28 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.720\n","\t Val. Loss: 0.754\n","\n","Valid loss improved from 0.7538 to 0.7402. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 29 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.711\n","\t Val. Loss: 0.740\n","\n","Valid loss improved from 0.7402 to 0.7292. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 30 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.698\n","\t Val. Loss: 0.729\n","\n","Epoch: 31 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.686\n","\t Val. Loss: 0.732\n","\n","Valid loss improved from 0.7292 to 0.7183. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 32 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.675\n","\t Val. Loss: 0.718\n","\n","Valid loss improved from 0.7183 to 0.7101. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 33 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.664\n","\t Val. Loss: 0.710\n","\n","Valid loss improved from 0.7101 to 0.7051. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 34 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.656\n","\t Val. Loss: 0.705\n","\n","Valid loss improved from 0.7051 to 0.7041. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 35 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.644\n","\t Val. Loss: 0.704\n","\n","Valid loss improved from 0.7041 to 0.6954. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 36 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.634\n","\t Val. Loss: 0.695\n","\n","Valid loss improved from 0.6954 to 0.6837. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 37 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.625\n","\t Val. Loss: 0.684\n","\n","Epoch: 38 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.614\n","\t Val. Loss: 0.689\n","\n","Valid loss improved from 0.6837 to 0.6783. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 39 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.605\n","\t Val. Loss: 0.678\n","\n","Valid loss improved from 0.6783 to 0.6686. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 40 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.596\n","\t Val. Loss: 0.669\n","\n","Epoch: 41 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.587\n","\t Val. Loss: 0.674\n","\n","Valid loss improved from 0.6686 to 0.6643. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 42 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.580\n","\t Val. Loss: 0.664\n","\n","Valid loss improved from 0.6643 to 0.6621. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 43 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.568\n","\t Val. Loss: 0.662\n","\n","Valid loss improved from 0.6621 to 0.6433. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 44 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.562\n","\t Val. Loss: 0.643\n","\n","Epoch: 45 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.554\n","\t Val. Loss: 0.646\n","\n","Valid loss improved from 0.6433 to 0.6401. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 46 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.543\n","\t Val. Loss: 0.640\n","\n","Valid loss improved from 0.6401 to 0.6324. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 47 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.536\n","\t Val. Loss: 0.632\n","\n","Valid loss improved from 0.6324 to 0.6249. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 48 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.530\n","\t Val. Loss: 0.625\n","\n","Valid loss improved from 0.6249 to 0.6187. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 49 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.522\n","\t Val. Loss: 0.619\n","\n","Valid loss improved from 0.6187 to 0.6157. Saving checkpoint: /content/drive/My Drive/files/checkpoint.pth\n","Epoch: 50 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.512\n","\t Val. Loss: 0.616\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"d9uUy36lZwp9"},"source":["# obtain new Database\n","Load the model\n","\n","Now, lets load save the model, so after saved in traning part  we can reload and make predicions without the need to retrain. The model is then converted to JSON format and written to model.json in the local directory. The network weights are written to model.pth in the local directory.\n"," Look at our files\n","\n","The model and weight data is loaded from the saved files and a new model is created. It is important to compile the loaded model before it is used. This is so that predictions made using the model can use the appropriate efficient computation from the pytorch backend.\n","\n","1.   create a new folder call it segemented_Classes\n","2.   create inside  that folder , each Class folder ( Normal , DiabeticRetinopathy , Myopia , gloumia )\n","3.    create a new folder call test to put all resized Classes folder resize the images Classes you have from origne dataset  into 512x512 to be compatible with input model Unet and the resized folder with name class\n","4.   finally step is , change the path file in test_x only for each segement class \n","\n"]},{"cell_type":"code","metadata":{"id":"rFwpuZWyA7jP"},"source":["import zipfile\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"id":"S32idgLVA7z_","executionInfo":{"elapsed":334,"status":"error","timestamp":1632658253482,"user":{"displayName":"Youness EL BRAG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtRxzu5DdV8ntFlalQyGLGS9UseGRNTl5JYMSPJg=s64","userId":"18435412878697928901"},"user_tz":-120},"outputId":"0df511f9-c563-43b5-a2f7-2f84e59c5074"},"source":["zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/train_s.zip\", 'r')\n","zip_ref.extractall(\"/content/drive/My Drive/test\")\n","zip_ref.close()\n"],"execution_count":null,"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-dc2707c3d489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzip_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/train_s.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'zipfile' is not defined"]}]},{"cell_type":"code","metadata":{"id":"laE3gccmDuVP"},"source":["# resize pokeGAN.py\n","import os\n","import cv2\n","\n","src = \"/content/drive/My Drive/train/Myopia\" #pokeRGB_black\n","dst = \"/content/drive/My Drive/test/class4\" # resized\n","\n","os.mkdir(dst)\n","\n","for each in os.listdir(src):\n","    img = cv2.imread(os.path.join(src,each))\n","    img = cv2.resize(img,(512,512))\n","    cv2.imwrite(os.path.join(dst,each), img)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":432},"id":"V8q3j6GcvezC","executionInfo":{"elapsed":1033,"status":"error","timestamp":1638534575022,"user":{"displayName":"Youness EL BRAG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtRxzu5DdV8ntFlalQyGLGS9UseGRNTl5JYMSPJg=s64","userId":"18435412878697928901"},"user_tz":0},"outputId":"f1ac245d-e8d6-48f9-e81f-5995168c9913"},"source":["import os, time\n","from operator import add\n","import numpy as np\n","from glob import glob\n","import cv2\n","from tqdm import tqdm\n","import imageio\n","import torch\n","from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n","\n","#from model import build_unet\n","#from utils import create_dir, seeding\n","\n","def calculate_metrics(y_true, y_pred):\n","    \"\"\" Ground truth \"\"\"\n","    y_true = y_true.cpu().numpy()\n","    y_true = y_true > 0.5\n","    y_true = y_true.astype(np.uint8)\n","    y_true = y_true.reshape(-1)\n","\n","    \"\"\" Prediction \"\"\"\n","    y_pred = y_pred.cpu().numpy()\n","    y_pred = y_pred > 0.5\n","    y_pred = y_pred.astype(np.uint8)\n","    y_pred = y_pred.reshape(-1)\n","\n","    score_jaccard = jaccard_score(y_true, y_pred)\n","    score_f1 = f1_score(y_true, y_pred)\n","    score_recall = recall_score(y_true, y_pred)\n","    score_precision = precision_score(y_true, y_pred)\n","    score_acc = accuracy_score(y_true, y_pred)\n","\n","    return [score_jaccard, score_f1, score_recall, score_precision, score_acc]\n","\n","def mask_parse(mask):\n","    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n","    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n","    return mask\n","\n","if __name__ == \"__main__\":\n","    \"\"\" Seeding \"\"\"\n","    seeding(42)\n","\n","    \"\"\" Folders \"\"\"\n","    create_dir(\"results\")\n","    # important step \n","    \"\"\" Load dataset and change path test_x when segement a new class just with name of class that have been resized \"\"\"\n","    test_x = sorted(glob(\"/content/drive/My Drive/test/class4/*\"))\n","    test_y = sorted(glob(\"/content/drive/My Drive/new_data/test/mask/*\"))\n","\n","    \"\"\" Hyperparameters \"\"\"\n","    H = 512\n","    W = 512\n","    size = (W, H)\n","    checkpoint_path = \"/content/drive/My Drive/files/checkpoint.pth\"\n","\n","    \"\"\" Load the checkpoint \"\"\"\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    model = build_unet()\n","    model = model.to(device)\n","    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n","    model.eval()\n","\n","    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0]\n","    time_taken = []\n","\n","    for i , (y,x)  in tqdm(enumerate(zip(test_x,test_x)), total=len(test_x)):\n","        \"\"\" Extract the name \"\"\"\n","        name = x.split(\"/\")[-1].split(\".\")[0]\n","\n","        \"\"\" Reading image \"\"\"\n","        image = cv2.imread(x, cv2.IMREAD_COLOR) ## (512, 512, 3)\n","        ## image = cv2.resize(image, size)\n","        x = np.transpose(image, (2, 0, 1))      ## (3, 512, 512)\n","        x = x/255.0\n","        x = np.expand_dims(x, axis=0)           ## (1, 3, 512, 512)\n","        x = x.astype(np.float32)\n","        x = torch.from_numpy(x)\n","        x = x.to(device)\n","\n","        with torch.no_grad():\n","            \"\"\" Prediction and Calculating FPS \"\"\"\n","            start_time = time.time()\n","            pred_y = model(x)\n","            pred_y = torch.sigmoid(pred_y)\n","            total_time = time.time() - start_time\n","            time_taken.append(total_time)\n","\n","        \"\"\" Saving masks \"\"\"\n","        #ori_mask = mask_parse(mask)\n","        pred_y = mask_parse(pred_y)\n","        line = np.ones((size[1], 10, 3)) * 128\n","        \n","        cat_images = np.concatenate(\n","            [pred_y * 255], axis=-1\n","        )\n","        cv2.imwrite(f\"/content/drive/My Drive/dataset/segemented_Classes/Myopia/{name}.png\", cat_images)\n","\n","    # jaccard = metrics_score[0]/len(test_x)\n","    # f1 = metrics_score[1]/len(test_x)\n","    # recall = metrics_score[2]/len(test_x)\n","    # precision = metrics_score[3]/len(test_x)\n","    # acc = metrics_score[4]/len(test_x)\n","    # print(f\"Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Recall: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f}\")\n","\n","    fps = 1/np.mean(time_taken)\n","    print(\"FPS: \", fps)"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/30 [00:00<?, ?it/s]\n"]},{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-286362c493fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\" Saving masks \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m#ori_mask = mask_parse(mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-52-286362c493fb>\u001b[0m in \u001b[0;36mmask_parse\u001b[0;34m(mask)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmask_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m## (512, 512, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## (512, 512, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mexpand_dims\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mexpand_dims\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."]}]}]}